{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baracus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_single_subject(df_bar_subj, to_int = True):\n",
    "    df_single_subject = pd.DataFrame(index=[x+1 for x in range(len(df_bar_subj))], columns = ['participant', 'aseg', 'area', 'thickness', 'stacked-anatomy'])\n",
    "    i = 0\n",
    "    # go through each participant\n",
    "    for p in df_bar_subj.subj_path:\n",
    "        tsv_file = [x for x in Path(p).glob('*.tsv')] # look at the predicted age tsv file that baracus outputs\n",
    "        if tsv_file:\n",
    "            df_temp = pd.read_csv(tsv_file[0], sep='\\t')\n",
    "            if to_int:\n",
    "                df_single_subject.set_value(i, 'participant', int((df_temp.subject_id[0])[4:])) # get participant label\n",
    "            else:\n",
    "                df_single_subject.set_value(i, 'participant', (df_temp.subject_id[0])[4:]) # get participant label\n",
    "            df_single_subject.set_value(i, 'aseg', df_temp.predicted_age[0]) # get aseg age prediction\n",
    "            df_single_subject.set_value(i, 'area', df_temp.predicted_age[1]) # get area age prediction\n",
    "            df_single_subject.set_value(i, 'thickness', df_temp.predicted_age[2]) # get thickness age prediction\n",
    "            df_single_subject.set_value(i, 'stacked-anatomy', df_temp.predicted_age[3]) # get stacked age prediction\n",
    "            i += 1\n",
    "    \n",
    "    return df_single_subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/migineishvilin2/python/envs/100runs/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "import pandas as pd\n",
    "\n",
    "def train_test_pipeline(df_features, df_target, test_size=0.3, random_state=None, model='no',model_train = None, model_test=None, nonlinear=None):\n",
    "    \n",
    "    if model=='no':\n",
    "        #split test and train data into equal parts\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df_features, df_target, test_size = test_size, random_state= random_state)\n",
    "    \n",
    "    else:\n",
    "        df_features = df_features.sort_index()\n",
    "        # get X_train\n",
    "        X_train = []\n",
    "        [X_train.append(df_features.iloc[x,:]) for x in model_train.index]\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "\n",
    "        # get corresponding y_train\n",
    "        y_train = pd.DataFrame([])\n",
    "        y_train = y_train.assign(age_at_scan = [df_target.iloc[x] for x in model_train.index])\n",
    "        y_train = y_train.set_index(model_train.index.values)\n",
    "        \n",
    "        # get X_test\n",
    "        X_test = []\n",
    "        [X_test.append(df_features.iloc[x,:]) for x in model_test.index]\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "\n",
    "        # get corresponding y_test\n",
    "        y_test = pd.DataFrame([])\n",
    "        y_test = y_test.assign(age_at_scan = [df_target.iloc[x] for x in model_test.index])\n",
    "        y_test = y_test.set_index(model_test.index.values)\n",
    "        \n",
    "    # imputer for missing values\n",
    "    fill_missing = Imputer()\n",
    "    var_thr = VarianceThreshold()\n",
    "    normalize = StandardScaler() # standard nomalizer\n",
    "\n",
    "    # create pipelist for sklearn transformations in order of specification\n",
    "    pipeline_list = []\n",
    "    pipeline_list = [('fill_missing', fill_missing),\n",
    "                     ('var_thr', var_thr),\n",
    "                     ('normalize', normalize)]\n",
    "    \n",
    "    # SVR regression model\n",
    "    if not nonlinear:\n",
    "        regression_model = SVR(kernel='linear', C=1.0, cache_size=1000)\n",
    "    elif nonlinear == 'poly':\n",
    "        regression_model = SVR(kernel=nonlinear, C=1.0, cache_size=1000)\n",
    "    else:\n",
    "        regression_model = SVR(kernel='rbf', C=1.0, cache_size=1000)\n",
    "    \n",
    "    pipeline_list.append(('regression_model', regression_model))\n",
    "    \n",
    "    # turn pipeline_list into sklearn Pipeline \n",
    "    pipe = Pipeline(pipeline_list)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pipe_fit(X_train, y_train):\n",
    "    # create pipelist for sklearn transformations in order of specification\n",
    "    # remove variance threshold because we're only using one feature\n",
    "    # imputer for missing values\n",
    "    fill_missing = Imputer()\n",
    "    var_thr = VarianceThreshold()\n",
    "    normalize = StandardScaler()\n",
    "    regression_model = SVR(kernel='linear', C=1.0, cache_size=1000)\n",
    "    \n",
    "    pipeline_list = []\n",
    "    pipeline_list = [('fill_missing', fill_missing),\n",
    "                 ('var_thr', var_thr),\n",
    "                 ('normalize', normalize),\n",
    "                 ('regression_model', regression_model)]\n",
    "    \n",
    "    pipe = Pipeline(pipeline_list)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction vs. Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# plots the train set performance on preduction v test set performance on prediction\n",
    "def plot_pred(y_pred_train, y_target_train, y_pred_test, y_target_test, fig_tuple=(10, 8), context='notebook'):\n",
    "    \n",
    "    import seaborn as sns\n",
    "    \n",
    "    sns.set_context(context, font_scale=1.2)\n",
    "    # plotting the Histogram\n",
    "    plt.figure(1)\n",
    "    plt.figure(figsize=fig_tuple)\n",
    "    \n",
    "    plt.plot(numpy.arange(0,80,0.1), numpy.arange(0,80,0.1), c='black', linewidth = 3)\n",
    "    plt.scatter(y_pred_train, y_target_train, c='b', marker = 'v', label = 'train')\n",
    "    plt.scatter(y_pred_test, y_target_test, color = 'r', marker = 'x', label='test')\n",
    "    plt.legend(loc = 'upper left')\n",
    "    plt.xlabel(\"predicted age\")\n",
    "    plt.ylabel(\"age\")\n",
    "    plt.xticks(np.arange(0,100,20))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/migineishvilin2/python/envs/100runs/lib/python3.6/site-packages/sklearn/learning_curve.py:23: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Validation Curve\n",
    "from sklearn.learning_curve import validation_curve\n",
    "import pylab as plt\n",
    "\n",
    "# Plots validation cure, i.e. C value versus Test Accuracy\n",
    "def plot_validation_curve(pipe, X_train, y_train):\n",
    "    param_range = np.logspace(-4, 2, num=18)\n",
    "    # fixme n_jobs\n",
    "    train_scores, test_scores = validation_curve(pipe, X_train, y_train, param_name=\"regression_model__C\",\n",
    "                                                     param_range=param_range)\n",
    "    \n",
    "    # plot\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Validation Curve\")\n",
    "    plt.xlabel(\"C\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0.0, 1.1)\n",
    "    plt.semilogx(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std,\n",
    "                         alpha=0.2,\n",
    "                         color=\"r\")\n",
    "    plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"g\")\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                         color=\"g\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot Learning Curve as more Training set samples get added\n",
    "def learning_curve_fct(X, y, out_name, estimator):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.learning_curve import learning_curve\n",
    "    import os, pickle\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import Imputer\n",
    "\n",
    "\n",
    "    def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                            n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "        plt.figure()\n",
    "        plt.title(title)\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "            \n",
    "        plt.xlabel(\"Training examples\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        \n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "        \n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        train_scores_std = np.std(train_scores, axis=1)\n",
    "        test_scores_mean = np.mean(test_scores, axis=1)\n",
    "        test_scores_std = np.std(test_scores, axis=1)\n",
    "        \n",
    "        plt.grid()\n",
    "        plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "        plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "\n",
    "        plt.legend(loc=\"best\")\n",
    "        return plt\n",
    "    \n",
    "    title = 'Learning Curves: ' + out_name\n",
    "    plot_learning_curve(estimator, title, X, y, cv=5, n_jobs=5, train_sizes=np.linspace(.5, 1.0, 5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baracus Data Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bar_plot(df_bar_subj, start_at=2, end_at=7):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # plot the predicted age v. real age of all modality predictions\n",
    "    plt.plot(np.arange(0,120,0.1), np.arange(0,120,0.1), c='black', linewidth = 3)\n",
    "    plt.scatter(df_bar_subj.iloc[:, start_at], df_bar_subj.iloc[:, end_at], c='b')\n",
    "    plt.scatter(df_bar_subj.iloc[:, start_at+1], df_bar_subj.iloc[:, end_at], c='r', marker='x')\n",
    "    plt.scatter(df_bar_subj.iloc[:, start_at+2], df_bar_subj.iloc[:, end_at], c='y', marker='s')\n",
    "    plt.scatter(df_bar_subj.iloc[:, start_at+3], df_bar_subj.iloc[:, end_at], c='g', marker='v')\n",
    "    plot_legend = plt.legend(loc = 'upper left')\n",
    "    plot_legend.get_texts()[0].set_text(\"aseg\")\n",
    "    plot_legend.get_texts()[1].set_text(\"area\")\n",
    "    plot_legend.get_texts()[2].set_text(\"thickness\")\n",
    "    plot_legend.get_texts()[3].set_text(\"stacked\")\n",
    "    plt.xlabel(\"predicted age\")\n",
    "    plt.ylabel(\"age\")\n",
    "    plt.xticks(np.arange(0,300,20))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freesurfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nibabel\n",
    "import numpy\n",
    "\n",
    "def vectorize_fs_surf(hemisphere_file, num_feat = None):\n",
    "    image = nibabel.load(hemisphere_file) # load file\n",
    "    sq_data = image.get_data().squeeze() # squeeze number of features\n",
    "    \n",
    "    if not num_feat:\n",
    "        vectorized_data = sq_data[numpy.newaxis, ...] # save the squeezed data to vectorized_data\n",
    "        assert vectorized_data.shape == (1, 2562) # assert that the new squeezed size\n",
    "    else:\n",
    "        sq_data = numpy.random.choice(sq_data, size= int((len(sq_data)*(0.75**num_feat))))\n",
    "        vectorized_data = sq_data[numpy.newaxis, ...]\n",
    "\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_fs_tab(aseg, num_feat = None):\n",
    "    df_aseg = pd.read_csv(aseg, index_col=0, delimiter='\\t') # read in aseg file\n",
    "    \n",
    "    if not num_feat:\n",
    "        vectorized_data = df_aseg.values # save volume values in vectorized_data\n",
    "        assert vectorized_data.shape == (1, 66) # assert that we have 66 volume measurements\n",
    "    else:\n",
    "        aseg_data = numpy.random.choice(df_aseg.values[0], size= int((len(df_aseg.values[0])*(0.75**num_feat))))\n",
    "        vectorized_data = [aseg_data]\n",
    "        \n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_surfaces(lh, rh, num_feat = None):\n",
    "    lh_data = vectorize_fs_surf(lh, num_feat) # get squeezed vectors\n",
    "    rh_data = vectorize_fs_surf(rh, num_feat)\n",
    "    return np.concatenate((lh_data, rh_data), 1) # combine left and right hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract thickness, ages or area data from the features\n",
    "def get_data(lh_thickness, rh_thickness, lh_area, rh_area, aseg, num_feat = None):\n",
    "    X = {}\n",
    "    X[\"thickness\"] = combine_surfaces(lh_thickness, rh_thickness, num_feat) # vectorize cortical thickness\n",
    "    X[\"area\"] = combine_surfaces(lh_area, rh_area, num_feat) # vectorize cortical surface area\n",
    "    X[\"aseg\"] = vectorize_fs_tab(aseg, num_feat)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(subject, lh_thickness_file, rh_thickness_file, lh_area_file, rh_area_file, aseg_file, num_feat=None):\n",
    "    X = get_data(lh_thickness_file, rh_thickness_file, lh_area_file, rh_area_file, aseg_file, num_feat = num_feat)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def subject_to_anal(subj_dirs, bids_bar):\n",
    "    subjects_to_analyze = []\n",
    "    # check baracus directory for extracted information from Freesurfer\n",
    "    # if extracted information exists for that subject, continue\n",
    "    # else, don't use that subject\n",
    "    for s in subj_dirs:\n",
    "        lh_thickness_file = os.path.join(bids_bar, s, \"data\", \"lh.thickness.mgh\")\n",
    "        rh_thickness_file = os.path.join(bids_bar, s, \"data\", \"rh.thickness.mgh\")\n",
    "        lh_area_file = os.path.join(bids_bar, s, \"data\", \"lh.area.mgh\")\n",
    "        rh_area_file = os.path.join(bids_bar, s, \"data\", \"rh.area.mgh\")\n",
    "        aseg_file = os.path.join(bids_bar, s, \"data\", \"aseg\")\n",
    "    \n",
    "        if os.path.isfile(lh_area_file) and os.path.isfile(rh_area_file) and os.path.isfile(lh_thickness_file) and os.path.isfile(rh_thickness_file) and aseg_file:\n",
    "            subjects_to_analyze.append(s)\n",
    "    \n",
    "    return subjects_to_analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(subjects_to_analyze, bids_bar, num_feat = None):\n",
    "    features = []\n",
    "    for s in subjects_to_analyze:\n",
    "        # these are guaranteed to exists, since we removed subjects without these files in the previous for loop\n",
    "        lh_thickness_file = os.path.join(bids_bar, s, \"data\", \"lh.thickness.mgh\")\n",
    "        rh_thickness_file = os.path.join(bids_bar, s, \"data\", \"rh.thickness.mgh\")\n",
    "        lh_area_file = os.path.join(bids_bar, s, \"data\", \"lh.area.mgh\")\n",
    "        rh_area_file = os.path.join(bids_bar, s, \"data\", \"rh.area.mgh\")\n",
    "        aseg_file = os.path.join(bids_bar, s, \"data\", \"aseg\")\n",
    "    \n",
    "        # get cortical thickness, cortical surface area and subcortical volumes\n",
    "        temp_X = get_features(s, lh_thickness_file, rh_thickness_file, lh_area_file, rh_area_file, aseg_file, num_feat)\n",
    "        temp_X[\"subj\"] = s\n",
    "        features.append(temp_X)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "def get_source_data(source, features, target, on='MASKID', is_string=False):\n",
    "    # Get Features\n",
    "    assert source == 'thickness' or source == 'area' or source == 'aseg'\n",
    "    arr_ = [x[source] for x in features]\n",
    "    arr_ = [x[0] for x in arr_]\n",
    "    narr_ = numpy.array(arr_) # covert into numpy array\n",
    "\n",
    "    df_data_ = pd.DataFrame(narr_)\n",
    "\n",
    "    if not is_string:\n",
    "        df_data_[on] = pd.Series([int(x['subj'][4:]) for x in features], index=df_data_.index) # add subject number to df\n",
    "    else:\n",
    "        df_data_[on] = pd.Series([x['subj'][4:13] for x in features], index=df_data_.index)\n",
    "    df_data_ = df_data_.merge(target, on=on) # add subject age to df\n",
    "    return df_data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stacked_ages(y_train, y_predicted_train, y_predicted_cv, y_test, y_predicted_test, first=False):\n",
    "    # stack predicted \n",
    "    \n",
    "    # merge predicted ct for train \n",
    "    df_y_train = pd.DataFrame([])\n",
    "    \n",
    "    if not first:\n",
    "        df_y_train = df_y_train.assign(age_at_scan = y_train.age_at_scan)\n",
    "    else:\n",
    "        df_y_train = df_y_train.assign(age_at_scan = y_train)\n",
    "    \n",
    "    df_y_train = df_y_train.set_index(y_train.index)\n",
    "    df_y_train = df_y_train.assign(pred_age_test = y_predicted_train)\n",
    "    df_y_train = df_y_train.assign(y_predicted_cv = y_predicted_cv)\n",
    "    df_y_train = df_y_train.assign(split_group = \"train\")\n",
    "    \n",
    "    # merge predicted ct for test\n",
    "    df_y_test = pd.DataFrame([])\n",
    "    if not first:\n",
    "        df_y_test = df_y_test.assign(age_at_scan = y_test.age_at_scan)\n",
    "    else:\n",
    "        df_y_test = df_y_test.assign(age_at_scan = y_test)\n",
    "    df_y_test = df_y_test.set_index(y_test.index)\n",
    "    df_y_test = df_y_test.assign(pred_age_test = y_predicted_test)\n",
    "    df_y_test = df_y_test.assign(split_group = \"test\")\n",
    "    \n",
    "    # concat ct test and ct train together\n",
    "    df_y = pd.concat([df_y_train, df_y_test])\n",
    "    \n",
    "    # sort by index to make sure we have the age predictions are in the right order,\n",
    "    # since they have been randomly shuffled\n",
    "    df_y = df_y.sort_index()\n",
    "    \n",
    "    return df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import cross_val_predict, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, median_absolute_error\n",
    "\n",
    "def tune_and_train_rf(X_train, y_train, target, strat_k_fold=None):\n",
    "    '''\n",
    "    Uses oob estimates to find optimal max_depth between None + 0...20\n",
    "    Refits with best max_depth\n",
    "    '''\n",
    "    oob_r2 = []\n",
    "    cv_list = [x for x in range(1, 20)] # different RF Depths\n",
    "    for md in cv_list:\n",
    "        # generate random forests and determine best depth\n",
    "        rf = RandomForestRegressor(n_estimators=100, max_depth=md, oob_score=True, random_state=0, n_jobs=-1)\n",
    "        rf.fit(X_train, y_train)\n",
    "        oob_r2.append(rf.oob_score_)\n",
    "\n",
    "    best_max_depth = cv_list[np.argmax(oob_r2)]\n",
    "    print(\"best max_depth: %s\" % best_max_depth)\n",
    "\n",
    "    # CV\n",
    "    # get random forest model\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=best_max_depth, oob_score=True, random_state=0, n_jobs=-1)\n",
    "    \n",
    "    cv_results = None\n",
    "    if strat_k_fold:\n",
    "        y_predicted_cv = cross_val_predict(rf, X_train, y_train, cv=strat_k_fold, n_jobs=-1)\n",
    "        cv_r2 = []\n",
    "        cv_mae = []\n",
    "        for k_train, k_test in strat_k_fold:\n",
    "            cv_r2.append(r2_score(y_train[k_test], y_predicted_cv[k_test]))\n",
    "            cv_mae.append(mean_absolute_error(y_train[k_test], y_predicted_cv[k_test]))\n",
    "        cv_results = {'y_predicted_cv': y_predicted_cv,\n",
    "                      'cv_r2': cv_r2,\n",
    "                      'cv_mae': cv_mae,\n",
    "                      'oob_r2': oob_r2}\n",
    "\n",
    "    # refit\n",
    "    rf.fit(X_train, y_train)\n",
    "    return rf, cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cross_validation import cross_val_predict, StratifiedKFold\n",
    "\n",
    "def stacking(source_dict, source_selection_dict, target, run_fitting=True, show=True):\n",
    "    df_in = {}\n",
    "\n",
    "    # create df_in for ct, ca and sv and add a source column to identify ct, ca or sv\n",
    "    for s, f in source_dict.items():\n",
    "        df_in[s] = f\n",
    "        df_in[s]['source'] = s\n",
    "        \n",
    "    # go though fs key\n",
    "    for stacking_crit in source_selection_dict.keys():\n",
    "        file_pref = target + '__' + stacking_crit + '__'\n",
    "        scores_test = pd.DataFrame([])\n",
    "        df_all = pd.DataFrame([]) # empty df\n",
    "    \n",
    "        # concatenate ct, ca, sv vertically\n",
    "        for s in source_selection_dict[stacking_crit]:\n",
    "            df_all = pd.concat((df_all, df_in[s]))\n",
    "    \n",
    "        # get one single source example to get age...\n",
    "        # gets ct because it's the last item in the key\n",
    "        df_single_source = df_in[s]\n",
    "    \n",
    "        # add columns in the case of test-only data\n",
    "        if 'split_group' not in df_all:\n",
    "            df_all['split_group'] = 'test'\n",
    "    \n",
    "        # add 'select', 'y_pred_cv', 'sample_weights', 'train_group_2samp' and 'study if those fields not already provided\n",
    "        for a in ['select', 'y_predicted_cv','sample_weights', 'train_group_2samp', 'study']:\n",
    "            if a not in df_all:\n",
    "                df_all[a] = np.nan\n",
    "    \n",
    "        # create a df that only takes certain columns of df_all\n",
    "        df = df_all[['source', 'age_at_scan', 'split_group', 'select', 'y_predicted_cv', 'pred_age_test', 'sample_weights', 'train_group_2samp', 'study']]\n",
    "        \n",
    "        \n",
    "        # separate df into test subjects\n",
    "        test_ind = df['split_group'] == 'test'\n",
    "        df_test = df[test_ind].copy()\n",
    "    \n",
    "        # run_fitting on train data \n",
    "        if run_fitting:  # fit rf\n",
    "            print ('Fitting stacking model')\n",
    "        \n",
    "            # split df into train subjects\n",
    "            train_ind = ((df['split_group'] == 'train') | (df['train_group_2samp'] == True))\n",
    "            df_train = df[train_ind].copy() \n",
    "            \n",
    "            # pivot table so that df_train displays index as rows, aseg, ca and ct as columns and y_pred_cv for corresponding values\n",
    "            dd_train = df_train.pivot_table(values='y_predicted_cv', columns='source', index=df_train.index)\n",
    "#             dd_train = pd.DataFrame([])\n",
    "#             dd_train = dd_train.assign(aseg = [df_train.y_predicted_cv[x] if df_train.source[x] == 'aseg' else None for x in df_train.index])\n",
    "            \n",
    "            # get ['aseg', 'ca', 'ct']\n",
    "            single_sources = dd_train.columns.values\n",
    "            # get mean y_pred across the three sources\n",
    "            dd_train['mean_pred'] = dd_train.mean(1)\n",
    "            # add actual age in\n",
    "            dd_train = dd_train.join(df_single_source[['age_at_scan']], how='left')\n",
    "        \n",
    "            # split into training features and target\n",
    "            X_train, y_train = dd_train[single_sources], dd_train['age_at_scan']\n",
    "        \n",
    "            # Call Tuning and Training Random Forest Function\n",
    "            rf, cv_results = tune_and_train_rf(X_train, y_train, target)\n",
    "        \n",
    "            # Plot Feature Importance\n",
    "            if show:\n",
    "                fi = pd.DataFrame(rf.feature_importances_, columns=['feature_importances'], index=single_sources)\n",
    "                plt.figure()\n",
    "                sns.barplot(fi.feature_importances, fi.index.values)\n",
    "                plt.xlim([0, 1])\n",
    "                plt.show()\n",
    "        \n",
    "            # Get Training Error\n",
    "            y_predicted_train_stack = rf.predict(X_train)\n",
    "            r2_train = r2_score(y_train, y_predicted_train_stack)\n",
    "            mae_train = mean_absolute_error(y_train, y_predicted_train_stack)\n",
    "            \n",
    "            dd_train['pred_age_train'] = y_predicted_train_stack\n",
    "            \n",
    "            if show:\n",
    "                plt.figure()\n",
    "                f = sns.jointplot('age_at_scan', 'pred_age_train', data=dd_train, xlim=(10, 90), ylim=(10, 90))\n",
    "                ax = sns.plt.gca()\n",
    "                plt.show()\n",
    "        else:\n",
    "            rf_file = rf_file_template.format(stacking_crit=stacking_crit)\n",
    "            rf = pickle.load(open(rf_file))\n",
    "            dd_train = pd.DataFrame([])\n",
    "        \n",
    "        # Prediction on Test Set\n",
    "        \n",
    "        # Pivot Test Subject so that columns = aseg, ca, ct and rows are the index of test subjects and values are predicted ages\n",
    "        dd_test = df_test.pivot_table(values='pred_age_test', columns='source', index=df_test.index)\n",
    "    \n",
    "        # get ['aseg', 'ca', 'ct']\n",
    "        single_sources = dd_test.columns.values\n",
    "    \n",
    "        dd_test['mean_pred'] = dd_test.mean(1) # calculate mean predicted age\n",
    "        dd_test = dd_test.join(df_single_source[['age_at_scan']], how='left') # add real age at the end of the df\n",
    "    \n",
    "        # separate into test features and test target\n",
    "        X_test, y_test = dd_test[single_sources], dd_test['age_at_scan']\n",
    "        dd_test['pred_age_test'] = rf.predict(X_test)\n",
    "    \n",
    "        for m in source_selection_dict[stacking_crit] + ['mean_pred', 'pred_age_test']:\n",
    "            scores_test.ix[m, 'r2'] = r2_score(dd_test['age_at_scan'], dd_test[m])\n",
    "            scores_test.ix[m, 'rpear'] = np.corrcoef(dd_test['age_at_scan'], dd_test[m])[0, 1]\n",
    "            scores_test.ix[m, 'rpear2'] = np.corrcoef(dd_test['age_at_scan'], dd_test[m])[0, 1] ** 2\n",
    "            scores_test.ix[m, 'mae'] = mean_absolute_error(dd_test['age_at_scan'], dd_test[m])\n",
    "            scores_test.ix[m, 'medae'] = median_absolute_error(dd_test['age_at_scan'], dd_test[m])\n",
    "            \n",
    "            if show:\n",
    "                plt.figure()\n",
    "                plt.scatter(dd_test['age_at_scan'], dd_test[m])\n",
    "                plt.plot([10, 90], [10, 90])\n",
    "                plt.xlim([10, 90])\n",
    "                plt.ylim([10, 90])\n",
    "                plt.title('predictions TEST: %s (%s)\\n%.3f' % (m, stacking_crit, scores_test.ix[m, 'r2']))\n",
    "                plt.gca().set_aspect('equal', adjustable='box')\n",
    "                plt.show()\n",
    "        \n",
    "        return scores_test, dd_train, dd_test, rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_anat_features(subjects_to_analyze, bids_bar):\n",
    "    white_matter = []\n",
    "    grey_matter = []\n",
    "    csf = []\n",
    "    intra_cran_vol = []\n",
    "\n",
    "    for s in subjects_to_analyze:\n",
    "        # calculate white matter\n",
    "        df_tempsub = pd.DataFrame.from_csv(bids_bar.joinpath(s + \"/data/aseg\").as_posix(), sep='\\t')\n",
    "        white_matter_ms = df_tempsub.iloc[0]['Left-Cerebellum-White-Matter'] + df_tempsub.iloc[0]['Right-Cerebellum-White-Matter'] + df_tempsub.iloc[0]['CorticalWhiteMatterVol']\n",
    "        white_matter.append(white_matter_ms)\n",
    "    \n",
    "        # calculate grey matter\n",
    "        grey_matter_ms = df_tempsub.iloc[0]['SubCortGrayVol']\n",
    "        grey_matter.append(grey_matter_ms)\n",
    "    \n",
    "        # calculate csf\n",
    "        csf_ms = df_tempsub.iloc[0]['BrainSegVol'] - df_tempsub.iloc[0]['BrainSegVolNotVent']\n",
    "        #csf_ms = df_tempsub.iloc[0]['CSF']\n",
    "        csf.append(csf_ms)\n",
    "    \n",
    "        # calculate intracranial volume\n",
    "        intra_cran_vol_ms = df_tempsub.iloc[0]['EstimatedTotalIntraCranialVol']\n",
    "        intra_cran_vol.append(intra_cran_vol_ms)\n",
    "        \n",
    "    return white_matter, grey_matter, csf, intra_cran_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_fraction(matter, total):\n",
    "    for indx in range(len(matter)):\n",
    "        matter[indx] = matter[indx]/total[indx]\n",
    "        \n",
    "    return matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "# Copy Files Over, then Run Baracus \n",
    "def copy_hcp_files(hcp_fs_surf_subj, hcp_fs_dir, hcp_fs_stats_subj):\n",
    "    for x in hcp_fs_surf_subj:\n",
    "        # get the right surf files\n",
    "        for f in ['lh.area', 'lh.sphere.reg', 'lh.thickness','rh.area', 'rh.sphere.reg', 'rh.thickness']:\n",
    "            out_dir = hcp_fs_dir.joinpath('sub-'+x[len(x)-11:len(x)-5]+'/surf/').as_posix() # out dir in NNDSP\n",
    "            # check if surf file exists, or if it has already been copied over\n",
    "            if (Path(x).joinpath(f)).exists() and not Path(out_dir).joinpath(f).exists():\n",
    "                print(\"Copying: \", Path(x).joinpath(f), \" to\", out_dir)\n",
    "                if not os.path.isdir(out_dir):\n",
    "                    os.makedirs(out_dir)\n",
    "                shutil.copy(Path(x).joinpath(f).as_posix() , out_dir)\n",
    "\n",
    "    for x in hcp_fs_stats_subj:\n",
    "        # get aseg files\n",
    "        for f in ['aseg.stats']:\n",
    "            out_dir = hcp_fs_dir.joinpath('sub-'+x[len(x)-12:len(x)-6]+'/stats/').as_posix() # out dir in NNDSP        \n",
    "            #check if aseg file exists, or if it has already been copied over\n",
    "            if (Path(x).joinpath(f)).exists() and not Path(out_dir).joinpath(f).exists():\n",
    "                print(\"Copying: \", Path(x).joinpath(f), \" to\", out_dir)\n",
    "                if not os.path.isdir(out_dir):\n",
    "                    os.makedirs(out_dir)\n",
    "                shutil.copy(Path(x).joinpath(f).as_posix() , out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "def print_mae(y_target, y_pred, model=\"\", run_type=''):\n",
    "    print(\"Mean Absolute Error\", model, run_type, mean_absolute_error(y_target, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "210px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "913px",
    "left": "0px",
    "right": "auto",
    "top": "106px",
    "width": "128px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
