{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Freesurfer Aseg File"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "metadata": {},
=======
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
   "outputs": [],
   "source": [
    "def vectorize_fs_tab(aseg, num_feat = None):\n",
    "    df_aseg = pd.read_csv(aseg, index_col=0, delimiter='\\t') # read in aseg file\n",
    "    \n",
    "    if not num_feat:\n",
    "        vectorized_data = df_aseg.values # save volume values in vectorized_data\n",
    "        assert vectorized_data.shape == (1, 66) # assert that we have 66 volume measurements\n",
    "    else:\n",
    "        aseg_data = numpy.random.choice(df_aseg.values[0], size= int((len(df_aseg.values[0])*(0.75**num_feat))))\n",
    "        vectorized_data = [aseg_data]\n",
    "        \n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorise Freesurfer Surf File"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
=======
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
   "outputs": [],
   "source": [
    "def vectorize_fs_surf(hemisphere_file, num_feat = None):\n",
    "    import nibabel\n",
    "    import numpy\n",
    "\n",
    "    image = nibabel.load(hemisphere_file) # load file\n",
    "    sq_data = image.get_data().squeeze() # squeeze number of features\n",
    "    \n",
    "    if not num_feat:\n",
    "        vectorized_data = sq_data[numpy.newaxis, ...] # save the squeezed data to vectorized_data\n",
    "        assert vectorized_data.shape == (1, 2562) # assert that the new squeezed size\n",
    "    else:\n",
    "        sq_data = numpy.random.choice(sq_data, size= int((len(sq_data)*(0.75**num_feat))))\n",
    "        vectorized_data = sq_data[numpy.newaxis, ...]\n",
    "\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Left & Right Hemispheres"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {},
=======
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
   "outputs": [],
   "source": [
    "def combine_surfaces(lh, rh, num_feat = None):\n",
    "    lh_data = vectorize_fs_surf(lh, num_feat) # get squeezed vectors\n",
    "    rh_data = vectorize_fs_surf(rh, num_feat)\n",
    "    return np.concatenate((lh_data, rh_data), 1) # combine left and right hemispheres"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "metadata": {},
=======
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
   "outputs": [],
   "source": [
    "# extract thickness, ages or area data from the features\n",
    "def get_data(lh_thickness, rh_thickness, lh_area, rh_area, aseg, num_feat = None):\n",
    "    X = {}\n",
    "    X[\"thickness\"] = combine_surfaces(lh_thickness, rh_thickness, num_feat) # vectorize cortical thickness\n",
    "    X[\"area\"] = combine_surfaces(lh_area, rh_area, num_feat) # vectorize cortical surface area\n",
    "    X[\"aseg\"] = vectorize_fs_tab(aseg, num_feat)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "metadata": {},
=======
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
   "outputs": [],
   "source": [
    "def get_features(subject, lh_thickness, rh_thickness, lh_area, rh_area, aseg, num_feat=None):\n",
    "    X = get_data(lh_thickness, rh_thickness, lh_area, rh_area, aseg, num_feat = num_feat)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {},
=======
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
   "outputs": [],
   "source": [
    "def subject_features(subjects, bar_dir, num_feat=None, session = ''):\n",
    "    import os\n",
    "    \n",
    "    subjects_to_analyze = []\n",
    "    features = []\n",
    "    # check baracus directory for extracted information from Freesurfer\n",
    "    # if extracted information exists for that subject, continue\n",
    "    # else, don't use that subject\n",
    "    for s in subjects:\n",
    "        \n",
    "        if 'ses-1' in s:\n",
    "            ses = ''\n",
    "        else:\n",
    "            ses = session\n",
    "        \n",
    "        lh_thickness_file = os.path.join(bar_dir, s+ses, \"data\", \"lh.thickness.mgh\")\n",
    "        rh_thickness_file = os.path.join(bar_dir, s+ses, \"data\", \"rh.thickness.mgh\")\n",
    "        lh_area_file = os.path.join(bar_dir, s+ses, \"data\", \"lh.area.mgh\")\n",
    "        rh_area_file = os.path.join(bar_dir, s+ses, \"data\", \"rh.area.mgh\")\n",
    "        aseg_file = os.path.join(bar_dir, s+ses, \"data\", \"aseg\")\n",
    "        \n",
    "        \n",
    "            \n",
    "        if (os.path.isfile(lh_area_file) and \n",
    "            os.path.isfile(rh_area_file) and \n",
    "            os.path.isfile(lh_thickness_file) and \n",
    "            os.path.isfile(rh_thickness_file) and \n",
    "            aseg_file):\n",
    "            \n",
    "            # add this the list of subjects we have data for\n",
    "            subjects_to_analyze.append(s)\n",
    "            \n",
    "            # extract its input features from the files\n",
    "            # get cortical thickness, cortical surface area and subcortical volumes\n",
    "            X_data = get_features(s, lh_thickness_file, rh_thickness_file, lh_area_file, rh_area_file, aseg_file, num_feat)\n",
    "            X_data[\"subj\"] = s\n",
    "            features.append(X_data)\n",
    "    \n",
    "    return subjects_to_analyze, features"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "metadata": {},
=======
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
   "outputs": [],
   "source": [
    "# get data\n",
    "def get_source_data(source, features, target, on='subject', is_int=True):\n",
    "    import numpy as np\n",
    "    # Get Features\n",
    "    assert source == 'thickness' or source == 'area' or source == 'aseg'\n",
    "    arr_ = [x[source] for x in features]\n",
    "    arr_ = [x[0] for x in arr_]\n",
    "    \n",
    "    narr_ = np.array(arr_) # covert into numpy array\n",
    "    \n",
    "    df_data_ = pd.DataFrame(narr_)\n",
    "    \n",
    "    if is_int:\n",
    "        df_data_[on] = pd.Series([int(re.search('sub-(\\d+)*', x['subj']).group(0)[4:]) for x in features], index=df_data_.index) # add subject number to df\n",
    "    else:\n",
    "        df_data_[on] = pd.Series([x['subj'][4:13] for x in features], index=df_data_.index)\n",
    "\n",
    "    df_data_ = pd.merge(df_data_, target, on=on) # add subject age to df\n",
    "       \n",
    "    return df_data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Simple Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fraction(matter, total):\n",
    "    for indx in range(len(matter)):\n",
    "        matter[indx] = matter[indx]/total[indx]\n",
    "        \n",
    "    return matter"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 42,
   "metadata": {},
=======
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
   "outputs": [],
   "source": [
    "def get_anat_features(subjects, bar_dir, session=''):\n",
    "    white_matter = []\n",
    "    grey_matter = []\n",
    "    csf = []\n",
    "    intra_cran_vol = []\n",
    "\n",
    "    for s in subjects:\n",
    "        \n",
    "        if 'ses' in s:\n",
    "            ses = ''\n",
    "        else:\n",
    "            ses = session\n",
    "        \n",
    "        aseg_file = bar_dir.joinpath(s + ses + \"/data/aseg\")\n",
    "        if aseg_file.exists():\n",
    "            # calculate white matter\n",
<<<<<<< HEAD
    "            df_tempsub = pd.read_csv(bar_dir.joinpath(s + ses + \"/data/aseg\").as_posix(), sep='\\t')\n",
=======
    "            df_tempsub = pd.DataFrame.from_csv(bar_dir.joinpath(s + ses + \"/data/aseg\").as_posix(), sep='\\t')\n",
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
    "            white_matter_ms = df_tempsub.iloc[0]['Left-Cerebellum-White-Matter'] + df_tempsub.iloc[0]['Right-Cerebellum-White-Matter'] + df_tempsub.iloc[0]['CorticalWhiteMatterVol']\n",
    "            white_matter.append(white_matter_ms)\n",
    "        \n",
    "            # calculate grey matter\n",
    "            grey_matter_ms = df_tempsub.iloc[0]['SubCortGrayVol']\n",
    "            grey_matter.append(grey_matter_ms)\n",
    "        \n",
    "            # calculate csf\n",
    "            csf_ms = df_tempsub.iloc[0]['BrainSegVol'] - df_tempsub.iloc[0]['BrainSegVolNotVent']\n",
    "            #csf_ms = df_tempsub.iloc[0]['CSF']\n",
    "            csf.append(csf_ms)\n",
    "        \n",
    "            # calculate intracranial volume\n",
    "            intra_cran_vol_ms = df_tempsub.iloc[0]['EstimatedTotalIntraCranialVol']\n",
    "            intra_cran_vol.append(intra_cran_vol_ms)\n",
    "    print(len(intra_cran_vol))\n",
    "            \n",
    "    # calculate white matter fraction\n",
    "    white_matter = calc_fraction(white_matter, intra_cran_vol)\n",
    "    # calculate grey matter fraction\n",
    "    grey_matter = calc_fraction(grey_matter, intra_cran_vol)\n",
    "    # calculate csf fraction\n",
    "    csf = calc_fraction(csf, intra_cran_vol)\n",
    "        \n",
    "    return white_matter, grey_matter, csf, intra_cran_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anal_subj(subjects, bar_dir, num_feat=None, session=''):\n",
    "    import os\n",
    "    \n",
    "    subjects_to_analyze = []\n",
    "    features = []\n",
    "    # check baracus directory for extracted information from Freesurfer\n",
    "    # if extracted information exists for that subject, continue\n",
    "    # else, don't use that subject\n",
    "    for s in subjects:\n",
    "        \n",
    "        if 'ses' in s:\n",
    "            ses = ''\n",
    "        else:\n",
    "            ses = session\n",
    "            \n",
    "        lh_thickness_file = os.path.join(bar_dir, s+ses, \"data\", \"lh.thickness.mgh\")\n",
    "        rh_thickness_file = os.path.join(bar_dir, s+ses, \"data\", \"rh.thickness.mgh\")\n",
    "        lh_area_file = os.path.join(bar_dir, s+ses, \"data\", \"lh.area.mgh\")\n",
    "        rh_area_file = os.path.join(bar_dir, s+ses, \"data\", \"rh.area.mgh\")\n",
    "        aseg_file = os.path.join(bar_dir, s+ses, \"data\", \"aseg\")\n",
    "    \n",
    "        if (os.path.isfile(lh_area_file) and \n",
    "            os.path.isfile(rh_area_file) and \n",
    "            os.path.isfile(lh_thickness_file) and \n",
    "            os.path.isfile(rh_thickness_file) and \n",
    "            aseg_file):\n",
    "            \n",
    "            # add this the list of subjects we have data for\n",
    "            subjects_to_analyze.append(s)\n",
    "    \n",
    "    return subjects_to_analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anat_features(df, bar_dir, is_int = True, session=''):\n",
    "    print(\"Finding Subjects\")\n",
    "    # get the subjects in the baracus directory\n",
    "    subject_paths = [os.path.basename(s) for s in df.subj_paths]\n",
    "    subjects_to_analyze = anal_subj(subject_paths, bar_dir, session = session)\n",
    "\n",
    "    \n",
    "    if is_int:\n",
    "        df_atf = pd.DataFrame([int(re.search('sub-(\\d+)*', x).group(0)[4:]) for x in subjects_to_analyze], columns=['subject'])\n",
    "    else:\n",
    "        df_atf = pd.DataFrame([x[4:4+len(df.subject.values[0])] for x in subjects_to_analyze], columns=['subject'])\n",
    "                \n",
    "    df_atf = df_atf.merge(df, on = 'subject')\n",
    "      \n",
    "    print(\"Extracting Features\")\n",
    "    wm, gm, csf, icv = get_anat_features(subject_paths, bar_dir, session = session)\n",
    "    \n",
    "    df_atf = df_atf.assign(Intra_Cran_Vol = icv)\n",
    "    df_atf = df_atf.assign(WM_Frac = wm)\n",
    "    df_atf = df_atf.assign(GM_Frac = gm)\n",
    "    df_atf = df_atf.assign(CSF_Frac = csf)\n",
    "    df_atf = df_atf.sort_values(by='subject')\n",
    "    \n",
    "    return df_atf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/migineishvilin2/python/envs/100runs/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
=======
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "import pandas as pd\n",
    "\n",
    "def train_test_pipeline(df_features, df_target, test_size=0.3, random_state=None, \n",
    "                        model=False, model_train = None, model_test=None, nonlinear=None):\n",
    "    \n",
    "    if not model:\n",
    "        #split test and train data into equal parts\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df_features, \n",
    "                                                            df_target, \n",
    "                                                            test_size = test_size, \n",
    "                                                            random_state= random_state)\n",
    "    \n",
    "    else:\n",
    "        df_features = df_features.sort_index()\n",
    "        # get X_train\n",
    "        X_train = []\n",
    "        [X_train.append(df_features.iloc[x,:]) for x in model_train.index]\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "\n",
    "        # get corresponding y_train\n",
    "        y_train = pd.DataFrame([])\n",
    "        y_train = y_train.assign(age = [df_target.iloc[x] for x in model_train.index])\n",
    "        y_train = y_train.set_index(model_train.index.values)\n",
    "        \n",
    "        # get X_test\n",
    "        X_test = []\n",
    "        [X_test.append(df_features.iloc[x,:]) for x in model_test.index]\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "\n",
    "        # get corresponding y_test\n",
    "        y_test = pd.DataFrame([])\n",
    "        y_test = y_test.assign(age = [df_target.iloc[x] for x in model_test.index])\n",
    "        y_test = y_test.set_index(model_test.index.values)\n",
    "        \n",
    "    # imputer for missing values\n",
    "    fill_missing = Imputer()\n",
    "    var_thr = VarianceThreshold()\n",
    "    normalize = StandardScaler() # standard nomalizer\n",
    "\n",
    "    # create pipelist for sklearn transformations in order of specification\n",
    "    pipeline_list = []\n",
    "    pipeline_list = [('fill_missing', fill_missing),\n",
    "                     ('var_thr', var_thr),\n",
    "                     ('normalize', normalize)]\n",
    "    \n",
    "    # SVR regression model\n",
    "    if not nonlinear:\n",
    "        regression_model = SVR(kernel='linear', C=1.0, cache_size=1000)\n",
    "    elif nonlinear == 'poly':\n",
    "        regression_model = SVR(kernel=nonlinear, C=1.0, cache_size=1000)\n",
    "    else:\n",
    "        regression_model = SVR(kernel='rbf', C=1.0, cache_size=1000)\n",
    "    \n",
    "    pipeline_list.append(('regression_model', regression_model))\n",
    "    \n",
    "    # turn pipeline_list into sklearn Pipeline \n",
    "    pipe = Pipeline(pipeline_list)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Training"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {},
=======
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
   "outputs": [],
   "source": [
    "def complex_source(source, features, target, feature_indx = 5124, target_indx = 5125, model=False, \n",
    "                   model_train=None, model_test=None, is_int=True):\n",
    "    from sklearn.cross_validation import cross_val_predict\n",
    "    from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "    df = get_source_data(source, features, target, is_int = is_int) # extract thickness info from features\n",
    "    df = df.sort_values(by='subject')\n",
    "    \n",
    "    \n",
    "    #split test and train data into equal parts\n",
    "    X_train, X_test, y_train, y_test, pipe = train_test_pipeline(df.iloc[:, :feature_indx],\n",
    "                                                                 df.iloc[:,target_indx], \n",
    "                                                                 test_size = 0.5, random_state=None,\n",
    "                                                                 model=model, model_train=model_train, \n",
    "                                                                 model_test=model_test)    \n",
    "    # fit model\n",
<<<<<<< HEAD
    "    pipe.fit(X=X_train, y=y_train.values.ravel())\n",
=======
    "    pipe.fit(X=X_train, y=y_train.as_matrix().ravel())\n",
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
    "    \n",
    "    # prediction and errors\n",
    "    predicted_train = pipe.predict(X_train)\n",
    "    predicted_test = pipe.predict(X_test)\n",
    "    \n",
    "    # cross val\n",
    "    predicted_cv = cross_val_predict(pipe, X_train, y_train.values.ravel())\n",
    "    \n",
    "    # test mean absolute error\n",
    "    print(\"Mean Absolute Error (Train, \" + source + \"):\", mean_absolute_error(y_train, predicted_train))\n",
    "    print(\"Mean Absolute Error (Test, \" + source + \"):\", mean_absolute_error(y_test, predicted_test))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, pipe, predicted_train, predicted_test, predicted_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacked_ages(y_train, y_predicted_train, y_predicted_cv, y_test, y_predicted_test, first=False):\n",
    "    # stack predicted \n",
    "    \n",
    "    # merge predicted ct for train \n",
    "    df_y_train = pd.DataFrame([])\n",
    "    \n",
    "    if not first:\n",
    "        df_y_train = df_y_train.assign(age = y_train.age)\n",
    "    else:\n",
    "        df_y_train = df_y_train.assign(age = y_train)\n",
    "    \n",
    "    df_y_train = df_y_train.set_index(y_train.index)\n",
    "    df_y_train = df_y_train.assign(pred_age_test = y_predicted_train)\n",
    "    df_y_train = df_y_train.assign(y_predicted_cv = y_predicted_cv)\n",
    "    df_y_train = df_y_train.assign(split_group = \"train\")\n",
    "    \n",
    "    # merge predicted ct for test\n",
    "    df_y_test = pd.DataFrame([])\n",
    "    if not first:\n",
    "        df_y_test = df_y_test.assign(age = y_test.age)\n",
    "    else:\n",
    "        df_y_test = df_y_test.assign(age = y_test)\n",
    "    df_y_test = df_y_test.set_index(y_test.index)\n",
    "    df_y_test = df_y_test.assign(pred_age_test = y_predicted_test)\n",
    "    df_y_test = df_y_test.assign(split_group = \"test\")\n",
    "    \n",
    "    # concat ct test and ct train together\n",
    "    df_y = pd.concat([df_y_train, df_y_test], sort=False)\n",
    "    \n",
    "    # sort by index to make sure we have the age predictions are in the right order,\n",
    "    # since they have been randomly shuffled\n",
    "    df_y = df_y.sort_index()\n",
    "    \n",
    "    return df_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack Singular SV Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import cross_val_predict, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, median_absolute_error\n",
    "\n",
    "def tune_and_train_rf(X_train, y_train, target, strat_k_fold=None):\n",
    "    '''\n",
    "    Uses oob estimates to find optimal max_depth between None + 0...20\n",
    "    Refits with best max_depth\n",
    "    '''\n",
    "    oob_r2 = []\n",
    "    cv_list = [x for x in range(1, 20)] # different RF Depths\n",
    "    for md in cv_list:\n",
    "        # generate random forests and determine best depth\n",
    "        rf = RandomForestRegressor(n_estimators=100, max_depth=md, oob_score=True, random_state=0, n_jobs=-1)\n",
    "        rf.fit(X_train, y_train)\n",
    "        oob_r2.append(rf.oob_score_)\n",
    "\n",
    "    best_max_depth = cv_list[np.argmax(oob_r2)]\n",
    "    print(\"best max_depth: %s\" % best_max_depth)\n",
    "\n",
    "    # CV\n",
    "    # get random forest model\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=best_max_depth, oob_score=True, random_state=0, n_jobs=-1)\n",
    "    \n",
    "    cv_results = None\n",
    "    if strat_k_fold:\n",
    "        y_predicted_cv = cross_val_predict(rf, X_train, y_train, cv=strat_k_fold, n_jobs=-1)\n",
    "        cv_r2 = []\n",
    "        cv_mae = []\n",
    "        for k_train, k_test in strat_k_fold:\n",
    "            cv_r2.append(r2_score(y_train[k_test], y_predicted_cv[k_test]))\n",
    "            cv_mae.append(mean_absolute_error(y_train[k_test], y_predicted_cv[k_test]))\n",
    "        cv_results = {'y_predicted_cv': y_predicted_cv,\n",
    "                      'cv_r2': cv_r2,\n",
    "                      'cv_mae': cv_mae,\n",
    "                      'oob_r2': oob_r2}\n",
    "\n",
    "    # refit\n",
    "    rf.fit(X_train, y_train)\n",
    "    return rf, cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cross_validation import cross_val_predict, StratifiedKFold\n",
    "\n",
    "def stacking(source_dict, source_selection_dict, target, run_fitting=True, show=True):\n",
    "    df_in = {}\n",
    "\n",
    "    # create df_in for ct, ca and sv and add a source column to identify ct, ca or sv\n",
    "    for s, f in source_dict.items():\n",
    "        df_in[s] = f\n",
    "        df_in[s]['source'] = s\n",
    "        \n",
    "    # go though fs key\n",
    "    for stacking_crit in source_selection_dict.keys():\n",
    "        file_pref = target + '__' + stacking_crit + '__'\n",
    "        scores_test = pd.DataFrame([])\n",
    "        df_all = pd.DataFrame([]) # empty df\n",
    "    \n",
    "        # concatenate ct, ca, sv vertically\n",
    "        for s in source_selection_dict[stacking_crit]:\n",
    "            df_all = pd.concat((df_all, df_in[s]), sort=False)\n",
    "    \n",
    "        # get one single source example to get age...\n",
    "        # gets ct because it's the last item in the key\n",
    "        df_single_source = df_in[s]\n",
    "    \n",
    "        # add columns in the case of test-only data\n",
    "        if 'split_group' not in df_all:\n",
    "            df_all['split_group'] = 'test'\n",
    "    \n",
    "        # add 'select', 'y_pred_cv', 'sample_weights', 'train_group_2samp' and 'study if those fields not already provided\n",
    "        for a in ['select', 'y_predicted_cv','sample_weights', 'train_group_2samp', 'study']:\n",
    "            if a not in df_all:\n",
    "                df_all[a] = np.nan\n",
    "    \n",
    "        # create a df that only takes certain columns of df_all\n",
    "        df = df_all[['source', 'age', 'split_group', 'select', 'y_predicted_cv', 'pred_age_test', 'sample_weights', 'train_group_2samp', 'study']]\n",
    "        \n",
    "        \n",
    "        # separate df into test subjects\n",
    "        test_ind = df['split_group'] == 'test'\n",
    "        df_test = df[test_ind].copy()\n",
    "    \n",
    "        # run_fitting on train data \n",
    "        if run_fitting:  # fit rf\n",
    "            print ('Fitting stacking model')\n",
    "        \n",
    "            # split df into train subjects\n",
    "            train_ind = ((df['split_group'] == 'train') | (df['train_group_2samp'] == True))\n",
    "            df_train = df[train_ind].copy() \n",
    "            \n",
    "            # pivot table so that df_train displays index as rows, aseg, ca and ct as columns and y_pred_cv for corresponding values\n",
    "            dd_train = df_train.pivot_table(values='y_predicted_cv', columns='source', index=df_train.index)\n",
    "#             dd_train = pd.DataFrame([])\n",
    "#             dd_train = dd_train.assign(aseg = [df_train.y_predicted_cv[x] if df_train.source[x] == 'aseg' else None for x in df_train.index])\n",
    "            \n",
    "            # get ['aseg', 'ca', 'ct']\n",
    "            single_sources = dd_train.columns.values\n",
    "            # get mean y_pred across the three sources\n",
    "            dd_train['mean_pred'] = dd_train.mean(1)\n",
    "            # add actual age in\n",
    "            dd_train = dd_train.join(df_single_source[['age']], how='left')\n",
    "        \n",
    "            # split into training features and target\n",
    "            X_train, y_train = dd_train[single_sources], dd_train['age']\n",
    "        \n",
    "            # Call Tuning and Training Random Forest Function\n",
    "            rf, cv_results = tune_and_train_rf(X_train, y_train, target)\n",
    "        \n",
    "            # Plot Feature Importance\n",
    "            if show:\n",
    "                fi = pd.DataFrame(rf.feature_importances_, columns=['feature_importances'], index=single_sources)\n",
    "                plt.figure()\n",
    "                sns.barplot(fi.feature_importances, fi.index.values)\n",
    "                plt.xlim([0, 1])\n",
    "                plt.show()\n",
    "        \n",
    "            # Get Training Error\n",
    "            y_predicted_train_stack = rf.predict(X_train)\n",
    "            r2_train = r2_score(y_train, y_predicted_train_stack)\n",
    "            mae_train = mean_absolute_error(y_train, y_predicted_train_stack)\n",
    "            \n",
    "            dd_train['pred_age_train'] = y_predicted_train_stack\n",
    "            \n",
    "            if show:\n",
    "                plt.figure()\n",
    "                f = sns.jointplot('age', 'pred_age_train', data=dd_train, xlim=(10, 90), ylim=(10, 90))\n",
    "                ax = sns.plt.gca()\n",
    "                plt.show()\n",
    "        else:\n",
    "            rf_file = rf_file_template.format(stacking_crit=stacking_crit)\n",
    "            rf = pickle.load(open(rf_file))\n",
    "            dd_train = pd.DataFrame([])\n",
    "        \n",
    "        # Prediction on Test Set\n",
    "        \n",
    "        # Pivot Test Subject so that columns = aseg, ca, ct and rows are the index of test subjects and values are predicted ages\n",
    "        dd_test = df_test.pivot_table(values='pred_age_test', columns='source', index=df_test.index)\n",
    "    \n",
    "        # get ['aseg', 'ca', 'ct']\n",
    "        single_sources = dd_test.columns.values\n",
    "    \n",
    "        dd_test['mean_pred'] = dd_test.mean(1) # calculate mean predicted age\n",
    "        dd_test = dd_test.join(df_single_source[['age']], how='left') # add real age at the end of the df\n",
    "    \n",
    "        # separate into test features and test target\n",
    "        X_test, y_test = dd_test[single_sources], dd_test['age']\n",
    "        dd_test['pred_age_test'] = rf.predict(X_test)\n",
    "    \n",
    "        for m in source_selection_dict[stacking_crit] + ['mean_pred', 'pred_age_test']:\n",
    "            scores_test.ix[m, 'r2'] = r2_score(dd_test['age'], dd_test[m])\n",
    "            scores_test.ix[m, 'rpear'] = np.corrcoef(dd_test['age'], dd_test[m])[0, 1]\n",
    "            scores_test.ix[m, 'rpear2'] = np.corrcoef(dd_test['age'], dd_test[m])[0, 1] ** 2\n",
    "            scores_test.ix[m, 'mae'] = mean_absolute_error(dd_test['age'], dd_test[m])\n",
    "            scores_test.ix[m, 'medae'] = median_absolute_error(dd_test['age'], dd_test[m])\n",
    "            \n",
    "            if show:\n",
    "                plt.figure()\n",
    "                plt.scatter(dd_test['age'], dd_test[m])\n",
    "                plt.plot([10, 90], [10, 90])\n",
    "                plt.xlim([10, 90])\n",
    "                plt.ylim([10, 90])\n",
    "                plt.title('predictions TEST: %s (%s)\\n%.3f' % (m, stacking_crit, scores_test.ix[m, 'r2']))\n",
    "                plt.gca().set_aspect('equal', adjustable='box')\n",
    "                plt.show()\n",
    "        \n",
    "        return scores_test, dd_train, dd_test, rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
   "metadata": {},
=======
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
   "outputs": [],
   "source": [
    "def complex_model(df, bar_dir, is_int=True, features = None, session = ''):\n",
    "    import os\n",
    "    from glob import glob\n",
    "    from sklearn.metrics import mean_absolute_error, r2_score\n",
    "    \n",
    "    if not features:\n",
    "        print(\"Finding Subjects\")\n",
    "        # get the subjects in the baracus directory\n",
    "        subject_paths = [os.path.basename(s) for s in df.subj_paths]\n",
    "\n",
    "        print(\"Extracting Features\")\n",
    "        subjects_to_analyze, features = subject_features(subject_paths, bar_dir, session = session)\n",
    "    \n",
    "    df_target = df[['subject', 'age']]\n",
    "    df_target = df_target.sort_values(by='subject')\n",
    "\n",
    "    print(\"\\nCortical Thickness Pipeline\")\n",
    "    #get calculations for cortical thickness\n",
    "    X_train_ct, X_test_ct, y_train_ct, y_test_ct, pipe_ct, predicted_train_ct, predicted_test_ct, predicted_cv_ct = complex_source('thickness', \n",
    "                                                                                                                                   features, df_target,\n",
    "                                                                                                                                  is_int=is_int,\n",
    "                                                                                                                                  )\n",
    "    \n",
    "    print(\"\\nSubcortival Volumes Pipeline\")\n",
    "    #get calculations for subcortical volumes\n",
    "    X_train_sv, X_test_sv, y_train_sv, y_test_sv, pipe_sv, predicted_train_sv, predicted_test_sv, predicted_cv_sv = complex_source('aseg', \n",
    "                                                                                                                                   features, \n",
    "                                                                                                                                   df_target, \n",
    "                                                                                                                                   feature_indx= 66, \n",
    "                                                                                                                                   target_indx = 67,\n",
    "                                                                                                                                   model=True, \n",
    "                                                                                                                                   model_train=X_train_ct,\n",
    "                                                                                                                                   model_test=X_test_ct, \n",
    "                                                                                                                                   is_int = is_int)\n",
    "    \n",
    "    print(\"\\nCortical Surface Area Pipeline\")\n",
    "    #get calculations for cortical surface area\n",
    "    X_train_ca, X_test_ca, y_train_ca, y_test_ca, pipe_ca, predicted_train_ca, predicted_test_ca, predicted_cv_ca = complex_source('area', \n",
    "                                                                                                                                   features, \n",
    "                                                                                                                                   df_target,\n",
    "                                                                                                                                   model=True, \n",
    "                                                                                                                                   model_train=X_train_ct,\n",
    "                                                                                                                                   model_test=X_test_ct,\n",
    "                                                                                                                                   is_int = is_int)\n",
    "    \n",
    "   \n",
    "    print(\"\\nStacking predictions\")\n",
    "    # stack predicted values\n",
    "    # get stacked age predictions (test and train) for ct\n",
    "    df_ct = get_stacked_ages(y_train_ct, predicted_train_ct, predicted_cv_ct, y_test_ct, predicted_test_ct, first=True)\n",
    "    # get stacked age prediction (test and train) for ca\n",
    "    df_ca = get_stacked_ages(y_train_ca, predicted_train_ca, predicted_cv_ca, y_test_ca, predicted_test_ca)\n",
    "    # get stacked age prediction (test and train) for sv\n",
    "    df_sv = get_stacked_ages(y_train_sv, predicted_train_sv, predicted_cv_sv, y_test_sv, predicted_test_sv)\n",
    "    \n",
    "    # stacking function inputes\n",
    "    target = 'age'\n",
    "    source_dict = {'aseg': df_sv, 'ct': df_ct,'ca': df_ca}\n",
    "    source_selection_dict = {'fs': ['aseg', 'ct', 'ca'],}\n",
    "    \n",
    "    print(\"Beginning Random Forest\")\n",
    "    scores_test, dd_train, dd_test, pipe_stack = stacking(source_dict, source_selection_dict, target, show=False)\n",
    "    \n",
    "    print(\"Mean Absolute Error (Train):\", mean_absolute_error(dd_train.age, dd_train.pred_age_train))\n",
    "    print(\"Mean Absolute Error (Test):\", mean_absolute_error(dd_test.age, dd_test.pred_age_test))\n",
    "    print(scores_test)\n",
    "    \n",
    "    pipes = [pipe_ct, pipe_sv, pipe_ca, pipe_stack]\n",
    "    \n",
    "    return scores_test, dd_train, dd_test, pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Complex Model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_test(df, bar_dir, pipes, feature_indx=[5124, 66, 5124], data='', is_int=True, \n",
    "                 features = None, session = '', pred = 0, pred_with_subj = 0):\n",
=======
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complex_test(df, bar_dir, pipes, feature_indx=[5124, 66, 5124], data='', is_int=True, \n",
    "                 features = None, session = '', pred = 0):\n",
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
    "    import os\n",
    "    from glob import glob\n",
    "    from sklearn.metrics import mean_absolute_error, r2_score\n",
    "    import re\n",
    "    \n",
    "    if not features:\n",
    "        print(\"Finding Subjects\")\n",
    "        # get the subjects in the baracus directory\n",
    "        subject_paths = [os.path.basename(s) for s in df.subj_paths]\n",
    "    \n",
    "        print(\"Extracting features\")\n",
    "        subjects_to_analyze, features = subject_features(subject_paths, bar_dir, session = session)\n",
    "    else:\n",
    "        subjects_to_analyze = anal_subj([os.path.basename(s) for s in df.subj_paths], bar_dir, session = session)\n",
    "    \n",
    "    print(len(subjects_to_analyze))\n",
    "    if len(subjects_to_analyze) == len(df):\n",
    "        df_target = df[['subject', 'age']]\n",
    "    else:\n",
    "        if is_int:\n",
<<<<<<< HEAD
    "            df_target = pd.DataFrame([int(re.search('sub-(\\d+)*', s).group(0)[4:]) for s in subjects_to_analyze], columns=['subject'])\n",
=======
    "            df_target = pd.DataFrame([int(re.search('sub-(\\d+)*', s).group(0)[4:]) for s in subjects_to_analyze], columns=[['subject']])\n",
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
    "        else:\n",
    "            df_target = pd.DataFrame([s[4:4+len(df.subject.values[0])] for s in subjects_to_analyze], columns=['subject'])\n",
    "        \n",
    "        df_target = df_target.merge(df, on='subject')\n",
    "        df_target = df_target[['subject', 'age']]\n",
    "    \n",
    "    print(len(df_target))\n",
    "    \n",
<<<<<<< HEAD
    "    df_preds = pd.DataFrame(df_target.subject, columns=['subject'])\n",
=======
    "    df_preds = pd.DataFrame(df_target.subject, columns=[['subject']])\n",
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
    "    sources = ['thickness', 'aseg', 'area']\n",
    "\n",
    "    print(\"Predicting from Pipelines\")\n",
    "    indx=0\n",
    "    for source in sources:\n",
    "        X = get_source_data(source, features, df_target, is_int = is_int) # extract thickness info from features\n",
    "        X = X.sort_values(by='subject')\n",
    "        \n",
    "        #print(X.head(), X.shape)\n",
    "        \n",
    "        predicted = pipes[indx].predict(X.iloc[:, :feature_indx[indx]])\n",
    "       # print(predicted)\n",
    "        \n",
    "        # test mean absolute error\n",
    "        print(\"Mean Absolute Error (\" + data + \" \" + source + \"):\", mean_absolute_error(df_target.age, predicted))\n",
    "        \n",
    "        df_preds[source] = predicted\n",
    "        indx=indx+1\n",
    "\n",
    "    predicted_stack = pipes[len(pipes)-1].predict(df_preds[sources])\n",
    "    \n",
    "    mae_stack = mean_absolute_error(df_target.age, predicted_stack)\n",
    "    # test mean absolute error\n",
    "    print(\"Mean Absolute Error (\" + data + \" stack):\", mae_stack)\n",
    "    \n",
    "    if pred == 1:\n",
    "        return predicted_stack \n",
<<<<<<< HEAD
    "    elif pred_with_subj == 1:\n",
    "        df_return = pd.DataFrame([])\n",
    "        df_return['subject'] = df_target['subject']\n",
    "        df_return['complex_pred'] = predicted_stack\n",
    "        return df_return\n",
=======
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
    "    elif pred == 2:\n",
    "        import numpy as np\n",
    "        return np.subtract(df_target.age, predicted_stack)\n",
    "    \n",
    "    return mae_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
   "metadata": {},
=======
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def anal_subj(subjects, bar_dir, num_feat=None, session=''):\n",
    "    import os\n",
    "    \n",
    "    subjects_to_analyze = []\n",
    "    features = []\n",
    "    # check baracus directory for extracted information from Freesurfer\n",
    "    # if extracted information exists for that subject, continue\n",
    "    # else, don't use that subject\n",
    "    for s in subjects:\n",
    "        \n",
    "        if 'ses' in s:\n",
    "            ses = ''\n",
    "        else:\n",
    "            ses = session\n",
    "            \n",
    "        lh_thickness_file = os.path.join(bar_dir, s+ses, \"data\", \"lh.thickness.mgh\")\n",
    "        rh_thickness_file = os.path.join(bar_dir, s+ses, \"data\", \"rh.thickness.mgh\")\n",
    "        lh_area_file = os.path.join(bar_dir, s+ses, \"data\", \"lh.area.mgh\")\n",
    "        rh_area_file = os.path.join(bar_dir, s+ses, \"data\", \"rh.area.mgh\")\n",
    "        aseg_file = os.path.join(bar_dir, s+ses, \"data\", \"aseg\")\n",
    "    \n",
    "        if (os.path.isfile(lh_area_file) and \n",
    "            os.path.isfile(rh_area_file) and \n",
    "            os.path.isfile(lh_thickness_file) and \n",
    "            os.path.isfile(rh_thickness_file) and \n",
    "            aseg_file):\n",
    "            \n",
    "            # add this the list of subjects we have data for\n",
    "            subjects_to_analyze.append(s)\n",
    "    \n",
    "    return subjects_to_analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
   "outputs": [],
   "source": [
    "def simple_model(df, bar_dir, is_int=True, model=False, model_train=None, model_test=None, session = ''):\n",
    "    import os\n",
    "    from glob import glob\n",
    "    from sklearn.metrics import mean_absolute_error, r2_score\n",
    "    import re\n",
    "    \n",
    "    print(\"Finding Subjects\")\n",
    "    # get the subjects in the baracus directory\n",
    "    subject_paths = [os.path.basename(s) for s in df.subj_paths]\n",
    "    subjects_to_analyze = anal_subj(subject_paths, bar_dir, session = session)\n",
    "\n",
    "    \n",
    "    if is_int:\n",
<<<<<<< HEAD
    "        df_atf = pd.DataFrame([int(re.search('sub-(\\d+)*', x).group(0)[4:]) for x in subjects_to_analyze], columns=['subject'])\n",
=======
    "        df_atf = pd.DataFrame([int(re.search('sub-(\\d+)*', x).group(0)[4:]) for x in subjects_to_analyze], columns=[['subject']])\n",
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
    "    else:\n",
    "        df_atf = pd.DataFrame([x[4:4+len(df.subject.values[0])] for x in subjects_to_analyze], columns=['subject'])\n",
    "                \n",
    "    df_atf = df_atf.merge(df, on = 'subject')\n",
    "      \n",
    "    print(\"Extracting Features\")\n",
    "    wm, gm, csf, icv = get_anat_features(subject_paths, bar_dir, session = session)\n",
    "    \n",
    "    df_atf = df_atf.assign(Intra_Cran_Vol = icv)\n",
    "    df_atf = df_atf.assign(WM_Frac = wm)\n",
    "    df_atf = df_atf.assign(GM_Frac = gm)\n",
    "    df_atf = df_atf.assign(CSF_Frac = csf)\n",
    "    df_atf = df_atf.sort_values(by='subject')\n",
    "    \n",
    "   \n",
    "    print(\"Training Model\")\n",
    "    if model:\n",
    "        X_train, X_test, y_train, y_test, pipe =  train_test_pipeline(df_atf.iloc[:, df.shape[1]:], \n",
    "                                                                                  df_atf.age, \n",
    "                                                                                  test_size = 0.5, \n",
    "                                                                                  random_state=None, \n",
    "                                                                                  model=True, \n",
    "                                                                                  model_train=model_train, \n",
    "                                                                                  model_test=model_test)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test, pipe =  train_test_pipeline(df_atf.iloc[:, df.shape[1]:], \n",
    "                                                                                  df_atf.age, \n",
    "                                                                                  test_size = 0.5, \n",
    "                                                                                  random_state=None, \n",
    "                                                                                  model=False)\n",
    "\n",
    "    pipe.fit(X=X_train, y=y_train.values.ravel())  \n",
    "    \n",
    "    predicted_train = pipe.predict(X_train)\n",
    "    predicted_test = pipe.predict(X_test)\n",
    "    \n",
    "    print(\"Mean Absolute Error (Train):\", mean_absolute_error(y_train, predicted_train))\n",
    "    print(\"Mean Absolute Error (Test):\", mean_absolute_error(y_test, predicted_test))\n",
    "    \n",
    "    return X_train, X_test, pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Simple Model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_test(df, bar_dir, pipe, is_int=True, data='', session = '', pred = 0, pred_with_subj = 0):\n",
=======
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_test(df, bar_dir, pipe, is_int=True, data='', session = '', pred = 0):\n",
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
    "    import os\n",
    "    from glob import glob\n",
    "    from sklearn.metrics import mean_absolute_error, r2_score\n",
    "    import re\n",
    "    \n",
    "    print(\"Finding Subjects\")\n",
    "    # get the subjects in the baracus directory\n",
    "    subject_paths = [os.path.basename(s) for s in df.subj_paths]\n",
    "    subjects_to_analyze = anal_subj(subject_paths, bar_dir, session = session)\n",
    "    \n",
    "    print(len(subjects_to_analyze))\n",
    "    if is_int:\n",
<<<<<<< HEAD
    "        df_atf = pd.DataFrame([int(re.search('sub-(\\d+)*', x).group(0)[4:]) for x in subjects_to_analyze], columns=['subject'])\n",
=======
    "        df_atf = pd.DataFrame([int(re.search('sub-(\\d+)*', x).group(0)[4:]) for x in subjects_to_analyze], columns=[['subject']])\n",
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
    "    else:\n",
    "        df_atf = pd.DataFrame([x[4:4+len(df.subject.values[0])] for x in subjects_to_analyze], columns=['subject'])\n",
    "        \n",
    "    df_atf = df_atf.merge(df, on = 'subject')\n",
    "    \n",
    "    print(len(df_atf))\n",
    "    \n",
    "    print(\"Extracting Features\")\n",
    "    wm, gm, csf, icv = get_anat_features(subject_paths, bar_dir, session = session)\n",
    "    \n",
    "    df_atf = df_atf.assign(Intra_Cran_Vol = icv)\n",
    "    df_atf = df_atf.assign(WM_Frac = wm)\n",
    "    df_atf = df_atf.assign(GM_Frac = gm)\n",
    "    df_atf = df_atf.assign(CSF_Frac = csf)\n",
    "    df_atf = df_atf.sort_values(by='subject')\n",
    "    \n",
    "    print(\"Predicting Pipes\")\n",
    "    predicted = pipe.predict(df_atf[['Intra_Cran_Vol', 'WM_Frac', 'GM_Frac', 'CSF_Frac']])\n",
    "    \n",
    "    mae = mean_absolute_error(df_atf.age, predicted)\n",
    "    print(\"Mean Absolute Error (\" + data + \" simple):\", mae)\n",
    "    \n",
    "    if pred == 1:\n",
    "        return predicted\n",
<<<<<<< HEAD
    "    elif pred_with_subj == 1:\n",
    "        df_return = pd.DataFrame([])\n",
    "        df_return['subject'] = df_atf['subject']\n",
    "        df_return['simple_pred'] = predicted\n",
    "        return df_return\n",
=======
>>>>>>> b715293e7227355d02759fd297e6ed120fb299d1
    "    elif pred == 2:\n",
    "        import numpy as np\n",
    "        return np.subtract(df_atf.age, predicted)\n",
    "    \n",
    "    return mae"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "246px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
